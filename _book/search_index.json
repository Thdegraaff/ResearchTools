[
["index.html", "Research Tools for Social Scientists 1 Preface", " Research Tools for Social Scientists Thomas de Graaff October 18, 2016 1 Preface I started this book as background material for the course Network Analysis and from the need to teach social science students (and Business Administration students in particular) the basics of R and the intuition behing applying linear regression. "],
["introduction.html", "2 Introduction 2.1 Why do we need all this? 2.2 Why use R and not other applications? 2.3 Regression analysis: not again!", " 2 Introduction 2.1 Why do we need all this? 2.2 Why use R and not other applications? 2.3 Regression analysis: not again! "],
["using-r-in-social-science-research.html", "3 Using `R’ in Social Science Research 3.1 Where to get it 3.2 How to use it 3.3 Reading in data 3.4 Regression modeling 3.5 Writing results 3.6 Making plots 3.7 Recap", " 3 Using `R’ in Social Science Research 3.1 Where to get it 3.2 How to use it 3.2.1 Scripts 3.2.2 Packages 3.3 Reading in data 3.4 Regression modeling 3.5 Writing results 3.6 Making plots 3.7 Recap "],
["linear-regression-and-how-to-apply-it.html", "4 Linear Regression and how to apply it 4.1 Theoretical background 4.2 Applications of linear regression", " 4 Linear Regression and how to apply it In the social sciences (in fact, in all sciences) linear regressions (also called OLS or ordinary least squares) or one of its relatives is the most used empirical research tool there is. Why? Is is relatively simple, computationally fast, easy to interpret and relies on relatively weak set of assumptions. Unfortunately, the assumptions needed to be able to apply linear regression, are often not well understood: both in the social sciences and beyond. Moreover, students in the social sciences typically get little guidance in how to apply linear regression in practice and how to interpret the results. Note that this does not have anything to do with the specific software students use, but more with that fact that regression techniques in a wide number of situations (courses) are just not give (apart from statistical or research methods courses). However, given the fact that data becomes increasingly more available, knowing when to use regression techniques, how to apply them and especially how to interpret and assess them is now becoming an issue of paramount importance. In this chapter, I will first focus in section 4.1 on the essential theory behind regression analysis. I really keep it to the bare minimum. But if you understand these basics, I would argue that you understand more than 75% of the theorical background (the rest if just nitty-gritty). Section 4.2 will focus on applications of linear regression, specification issues (how many variables) and how to interpret the results 4.1 Theoretical background This subsection first deals with the model (what are you trying to model), thes about the three critical assumptions of (ordinary) least squares, discusses subsequently typical situations when these assumptions are violated, and finishes with a discussion about less important stuff (on which, alas, quite some attention is given in bachelor courses). Before we start, I would like to make one important remark. In general, statistical models can be used for (i) findin statisical relations, finding (ii) causal relations and for (iii) predicting. All three uses require the same assumptions, but have different focuses. In statistical, generally the focus is on finding statistical relations, such as whether the Dutch population is taller then the German popoulation. In economics the focus is very much on finding causal relations, so the need for explanatory power is not very large. Models that do not explain much (where the \\(R^2\\)’s are low, say \\(&lt;\\) 0.2) are just as good as models that explain quite a lot, as long as the least squares assumptions hold. In transportation science in general (and other disciplines that deals with making large models) predictions and thus eplanatory power is key. Here it is now very important that you perfectly understand what causes what as long as out-of-sample predicting is good (say for predicting future commuting flows). I usually have finding causal relations in mind when talking about least squares (already difficult enough), but note again that the same least squares assumptions should hold when you want to predict or want to find statistical relations. 4.1.1 The model Assume we are interested in the effect of the weight of a car on the fuel efficiency of the car (measures in miles per gallon). We have found a dataset with 32 observations and state the following univariate regression model: \\[ y_i = \\alpha + \\beta x_i + \\epsilon_i, \\] where \\(y\\) denotes the fuel efficiency of the car, \\(x\\) denotes the weight of the car and subscript \\(i\\) stands for the \\(i\\)-th observation. \\(\\alpha\\) and \\(\\beta\\) are the parameters of the model and they are unknown so have to be estimated. \\(\\epsilon\\) is a so-called error term and denotes all the variation that is not captures by our two variables (\\(\\alpha\\) and \\(\\beta\\)) and our weight variable \\(x\\). The following observations are rather important: What is on the left hand side of the \\(=\\) sign is what is to be explained (in this case miles per gallon). What is on the right hand side is what we use to explain \\(y\\). In this case \\(x\\), the weight of the car. The parameters \\(\\alpha\\) en \\(\\beta\\) constitute a linear relation between \\(x\\) and \\(y\\) The parameter \\(\\alpha\\) is the constant and denotes where the linear relation crosses the \\(y\\)-axis. \\(\\beta\\) gives the impact of \\(x\\) on \\(y\\). Because it is a linear relation, the effect is simple. One unit change of \\(x\\) is associated with a \\(\\beta\\) change of \\(y\\). In general, we can say that \\(\\beta\\) is equal to the marginal effect (\\(\\frac{\\partial y}{\\partial x}\\)). The regression error term \\(\\epsilon\\) gives all variation that is not captured by our model, so \\(y_i -(\\alpha + \\beta x_i) = \\epsilon_i\\). In this case, weight of the car most likely does not capture all variation in miles per gallon, so quite some variation is left in \\(\\epsilon\\). Something else is captured as well by \\(\\epsilon\\) and that is the measurement error of \\(y\\). So if we have not measured miles per gallon precisely enough then that variation is captured as well by \\(\\epsilon\\). Now, let’s assume that we to incorporate another variable (say the number of cylinders \\(c\\)), because we think that that variable is very important in explaining miles per gallon. Then we get the following multivariate regression model: \\[ y_i = \\alpha + \\beta_1 x_i + \\beta_2 c_i + \\epsilon_i, \\] where we now have two variables on the right hand side (\\(x_i\\) and \\(c_i\\)) and three parameters (\\(\\alpha\\), \\(\\beta_1\\) and \\(\\beta_2\\)). In effect nothing changes with the intuition behind the model. Except for the interpretation of the parameter \\(\\beta_1\\) (and thus also \\(\\beta_2\\)). Parameter \\(\\beta_1\\) now measures the impact of \\(x\\) on \\(y\\) holding \\(c\\) constant. So, multivariate regression models is nothing more (and less) than controlling for other factors. And we see later why that is very important. 4.1.2 The least squares assumptions We are interested in the effect of the weight of the car on fuel efficiency and therefore, our estimate of \\(\\beta_1\\) should be very close to the true \\(\\beta_1\\), especially when we have a large number of observations. Regression is great and utterly brilliant in finding this estimate, as long as the following three least squares assumptions hold: There are no large outliers All left hand side (in this case \\(y\\)) and right side variables (in this case \\(x\\) and \\(c\\)) are i.i.d. For the error term the following must hold: \\(E[\\epsilon|X=x] = 0\\). The first one is easy to understand. OLS is very sensitive to large outliers in the dataset. It is therefore always good to look for outliers and think whether they are real observations or perhaps typo’s (in Excel or something). Do not throw outliers immediately away but check whether they are correct. The second is relatively easy to understand as well (but not that much in practice). i.i.d. in this case stands for independently and identically distributed. This basically means that the observations in your dataset are independent from each other: in other words, the observations should have been correctly sampled. The third looks the most horrible, and, to be quite honest, is so–both in theory as in reality. This is also the assummption that is least well understood. And especially in assessing whether regression output is correct (is your estimate really close to \\(\\beta_1\\)) this assumption is crucial. 4.1.3 Possible violations and how to spot them A violation of the first assumption is usually easy to spot. There is a very strange outlier. But this also marks the importance of analysing descriptive statistics, including means, maximums and minimums, scatterplots and histograms. Whether your data is not i.i.d. can come because of a couple of reasons. The most straightforward is getting your data via snowballing (asking your friends and families using facebook to fill in a questionnaire and to ask their friends and families to do so as well). Usually, this means that you have a very specific sample and that the estimate you get is not close to the true value for the whole population. Observations might also be dependent upon each other, because of unobserved factors. In our case, it might be that a type of cars (American) are less fuel efficient than other cars (European). Another typical violation of the i.i.d. assumption is in time-series, where what happened in the previous period might have a large effect on the current period. In general, however, violations of the i.i.d. property are not that devestating for your model as long as you are only interested in finding the true \\(\\beta_1\\): namely, is usually only affects the precision of your estimate (the standard error) and not the estimate itself. When this assumption is violated, we therefore say that the estimate is inefficient. When the third assumption is violated, we say that our estimate is biased, in other words wrong: our parameter estimate does not come close to the true parameter. And this happens more often than not. So, what does \\(E[\\epsilon|X=x] = 0\\) actually mean. Loosely speaking, the parameters on the right hand side (in our case \\(x\\) and \\(c\\)) and the error term (\\(\\epsilon\\)) are not allowed to be correlated. There are several ways that this might happen, from which the following are in our case the most relevant: Reverse causality: \\(x\\) impacts \\(y\\), but \\(y\\) might impact \\(x\\) as well. This is the classical chicken and egg problem. Do firm perform better because of good management, or do good managers go to the better performing firms? Unobserved heterogeneity bias: there are factor that not in the model but influence both \\(x\\) and \\(y\\). For example, if american cars have both an influence on fuel efficiency and weight of the cars then the estimate that we find is not close to the true value of \\(\\beta_1\\). Misspecification: we assume that our model is linear, but it actually is not. Then, again, our estimate that we find is not close to the true value of \\(\\beta_1\\). There are other sources of violations, but in this case, these are the most important ones. Reverse causality is usually hard to correct for, but unobserved heterogeneity bias is luckily easier. Namely, we add relevant control variables (as we did with \\(c\\)). In this case we can minimize possible unobserved heterogeity bias. Misspefications are in general as well relative easy to correct for. From our descriptive statistics and scatterplot we can usually can infer the relation between \\(x\\) and \\(y\\) and control for possible nonlinearities by using (natural) logarithms and squared terms. Natural logarithms are the ones most used for various reasons not discussed here. If we take the logarithm of both sides then for our univariate regression we get: \\[ \\ln(y_i) = \\alpha + \\beta \\ln(x_i) + \\epsilon_i, \\] and all the aforementioned rules and assumptions still apply. But there is something peculiar to this regression. Namely, if we are interested in the marginal effect (\\(\\frac{\\partial y}{\\partial x}\\)), we get the following: \\[ \\frac{\\partial y}{\\partial x} = \\frac{\\partial e^{\\ln(y_t)}}{\\partial x} = \\frac{\\partial e^{(\\alpha + \\beta \\ln(x_i) + \\epsilon_i)}}{\\partial x} = \\frac{\\beta}{x}e^{(\\alpha + \\beta \\ln(x_i) + \\epsilon_i)} = \\frac{\\beta}{x}y, \\] In other words: \\[ \\beta = \\frac{\\partial y}{\\partial x} \\frac{x}{y} \\] which is simply the elasticity between \\(x\\) and \\(y\\). So, if there are logarithm on both the left and right hand side then the parameters (the \\(\\beta\\)’s) denotes elasticities. 4.1.4 Normality, heteroskedasticity and multicollinearity Until now, we have not discussed the concept normality, heteroskedasticity and multicollinearity. That is because they are not that relevant. OLS hinges upon its three assumptions (and they are already difficult enough). In fact, if the three assumptions are satisfied, then, as an outcome, the parameters are normally distributed. It goes to far to explain why (the theorems required for this are deeply fundamental to statistics), but in any case, normality is not a core OLS assumptions. It would be nice if both \\(y\\) and \\(x\\) are normally distributed because then the standard errors are minimized, but again, whether the estimate of \\(\\beta\\) you find is correct or not (biased or unbiased) does not depend on normality assumptions. Heteroskedasticy (or standard errors that are not constant) as well leads to quite some confusion. In general heteroskedasticity only leads to inefficient estimates (so only affects the standard errors). Nothing more, nothing less. And there are corrections for that (robust standard errors in Stata and similar procedures in R), so that nobody needs to cary anymore about heteroskedasticity. Finally, multicollinearity. 4.2 Applications of linear regression "],
["network-analysis-with-r.html", "5 Network analysis with R", " 5 Network analysis with R "],
["in-conclusion.html", "6 In conclusion", " 6 In conclusion "]
]

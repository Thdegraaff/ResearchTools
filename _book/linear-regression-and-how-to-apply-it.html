<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Research Tools for Social Scientists</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Research Tools for Social Scientists">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="Research Tools for Social Scientists" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Research Tools for Social Scientists" />
  
  
  

<meta name="author" content="Thomas de Graaff">

<meta name="date" content="2016-10-18">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="using-r-in-social-science-research.html">
<link rel="next" href="network-analysis-with-r.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#why-do-we-need-all-this"><i class="fa fa-check"></i><b>2.1</b> Why do we need all this?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#why-use-r-and-not-other-applications"><i class="fa fa-check"></i><b>2.2</b> Why use <code>R</code> and not other applications?</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#regression-analysis-not-again"><i class="fa fa-check"></i><b>2.3</b> Regression analysis: not again!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html"><i class="fa fa-check"></i><b>3</b> Using <code>R</code> in Social Science Research</a><ul>
<li class="chapter" data-level="3.1" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#where-to-get-it"><i class="fa fa-check"></i><b>3.1</b> Where to get it</a></li>
<li class="chapter" data-level="3.2" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#how-to-use-it"><i class="fa fa-check"></i><b>3.2</b> How to use it</a><ul>
<li class="chapter" data-level="3.2.1" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#scripts"><i class="fa fa-check"></i><b>3.2.1</b> Scripts</a></li>
<li class="chapter" data-level="3.2.2" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#packages"><i class="fa fa-check"></i><b>3.2.2</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#reading-in-data"><i class="fa fa-check"></i><b>3.3</b> Reading in data</a></li>
<li class="chapter" data-level="3.4" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#regression-modeling"><i class="fa fa-check"></i><b>3.4</b> Regression modeling</a></li>
<li class="chapter" data-level="3.5" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#writing-results"><i class="fa fa-check"></i><b>3.5</b> Writing results</a></li>
<li class="chapter" data-level="3.6" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#making-plots"><i class="fa fa-check"></i><b>3.6</b> Making plots</a></li>
<li class="chapter" data-level="3.7" data-path="using-r-in-social-science-research.html"><a href="using-r-in-social-science-research.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html"><i class="fa fa-check"></i><b>4</b> Linear Regression and how to apply it</a><ul>
<li class="chapter" data-level="4.1" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html#sec:theory"><i class="fa fa-check"></i><b>4.1</b> Theoretical background</a><ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html#the-model"><i class="fa fa-check"></i><b>4.1.1</b> The model</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html#the-least-squares-assumptions"><i class="fa fa-check"></i><b>4.1.2</b> The least squares assumptions</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html#possible-violations-and-how-to-spot-them"><i class="fa fa-check"></i><b>4.1.3</b> Possible violations and how to spot them</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html#normality-heteroskedasticity-and-multicollinearity"><i class="fa fa-check"></i><b>4.1.4</b> Normality, heteroskedasticity and multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression-and-how-to-apply-it.html"><a href="linear-regression-and-how-to-apply-it.html#sec:applications"><i class="fa fa-check"></i><b>4.2</b> Applications of linear regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="network-analysis-with-r.html"><a href="network-analysis-with-r.html"><i class="fa fa-check"></i><b>5</b> Network analysis with <code>R</code></a></li>
<li class="chapter" data-level="6" data-path="in-conclusion.html"><a href="in-conclusion.html"><i class="fa fa-check"></i><b>6</b> In conclusion</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Research Tools for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-and-how-to-apply-it" class="section level1">
<h1><span class="header-section-number">4</span> Linear Regression and how to apply it</h1>
<p>In the social sciences (in fact, in all sciences) linear regressions (also called OLS or ordinary least squares) or one of its relatives is the most <em>used</em> empirical research tool there is. Why? It is relatively simple, computationally fast, easy to interpret and relies on a relatively weak set of assumptions. Unfortunately, the assumptions needed to be able to apply linear regression, are often not well understood: both in the social sciences and beyond. Moreover, students in the social sciences typically get little guidance in how to apply linear regression in practice and how to interpret the results. Note that this does not have anything to do with the specific software students use, but more with that fact that regression techniques in a wide number of situations (courses) are just not given (apart from statistical or research methods courses). However, given the fact that data becomes increasingly more available, knowing when to use regression techniques, how to apply them and especially how to interpret and assess them is now becoming an issue of paramount importance. Or, perhaps more compelling, you need them to write your thesis.</p>
<p>In this chapter, I will first focus in section <a href="linear-regression-and-how-to-apply-it.html#sec:theory">4.1</a> on the <em>essential</em> theory behind regression analysis. I really keep it to the bare minimum. But if you understand these basics, I would argue that you understand more than 75% of the theorical background (the rest if just nitty-gritty). Section <a href="linear-regression-and-how-to-apply-it.html#sec:applications">4.2</a> will focus on applications of linear regression, specification issues (how many variables) and how to interpret the results.</p>
<div id="sec:theory" class="section level2">
<h2><span class="header-section-number">4.1</span> Theoretical background</h2>
<p>This subsection first deals with the model (what are you trying to explain), then about the three critical assumptions of (ordinary) least squares, discusses subsequently typical situations when these assumptions are violated, and finishes with a discussion about less important stuff (on which, alas, quite some attention is given in bachelor courses).</p>
<p>Before we start, I would like to make one important remark. In general, statistical models can be used for (<em>i</em>) finding <strong>statistical</strong> relations, finding (<em>ii</em>) <strong>causal</strong> relations and for (<em>iii</em>) <strong>predicting</strong>. All three uses require the same assumptions, but have different focuses. In statistical, generally the focus is on finding statistical relations, such as whether the Dutch population is taller then the German population. In economics the focus is very much on finding causal relations, so the need for explanatory power is not very large. Models that do not explain much (where the <span class="math inline">\(R^2\)</span>’s are low, say <span class="math inline">\(&lt;\)</span> 0.2) are just as good as models that explain quite a lot, as long as the least squares assumptions hold. In transportation science in general (and other disciplines that deals with making large models) predictions and thus eplanatory power is key. Here it is now very important that you perfectly understand what causes what as long as out-of-sample predicting is good (say for predicting future commuting flows).</p>
<p>I usually have finding causal relations in mind when talking about least squares (already difficult enough), but note again that the same least squares assumptions should hold when you want to predict or want to find statistical relations.</p>
<div id="the-model" class="section level3">
<h3><span class="header-section-number">4.1.1</span> The model</h3>
<p>Assume we are interested in the effect of the weight of a car on the fuel efficiency of the car (measures in miles per gallon). We have found a dataset with 32 observations and state the following univariate regression model: <span class="math display">\[
y_i = \alpha + \beta x_i + \epsilon_i,
\]</span> where <span class="math inline">\(y\)</span> denotes the fuel efficiency of the car, <span class="math inline">\(x\)</span> denotes the weight of the car and subscript <span class="math inline">\(i\)</span> stands for the <span class="math inline">\(i\)</span>-th observation. <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the parameters of the model and they are <strong>unknown</strong> so have to be <strong>estimated</strong>. <span class="math inline">\(\epsilon\)</span> is a so-called error term and denotes all the variation that is not captures by our two variables (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) and our weight variable <span class="math inline">\(x\)</span>.</p>
<p>The following observations are rather important:</p>
<ol style="list-style-type: decimal">
<li>What is on the left hand side of the <span class="math inline">\(=\)</span> sign is what is to be explained (in this case miles per gallon). What is on the right hand side is what we use to explain <span class="math inline">\(y\)</span>. In this case <span class="math inline">\(x\)</span>, the weight of the car.</li>
<li>The parameters <span class="math inline">\(\alpha\)</span> en <span class="math inline">\(\beta\)</span> constitute a <strong>linear</strong> relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li>The parameter <span class="math inline">\(\alpha\)</span> is the constant and in the univariate setting denotes where the linear relation crosses the <span class="math inline">\(y\)</span>-axis.</li>
<li><span class="math inline">\(\beta\)</span> gives the impact of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>. Because it is a linear relation, the effect is simple. One unit change of <span class="math inline">\(x\)</span> is associated with a <span class="math inline">\(\beta\)</span> change of <span class="math inline">\(y\)</span>. In general, we can say that <span class="math inline">\(\beta\)</span> is equal to the marginal effect (<span class="math inline">\(\frac{\partial y}{\partial x}\)</span>). Moreover, in a univariate setting <span class="math inline">\(\beta\)</span> denote the slope of the relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</li>
<li>The regression error term <span class="math inline">\(\epsilon\)</span> gives all variation that is not captured by our model, so <span class="math inline">\(y_i -(\alpha + \beta x_i) = \epsilon_i\)</span>. In this case, weight of the car most likely does not capture all variation in miles per gallon, so quite some variation is left in <span class="math inline">\(\epsilon\)</span>. Something else is captured as well by <span class="math inline">\(\epsilon\)</span> and that is the measurement error of <span class="math inline">\(y\)</span>. So, if we have not measured miles per gallon precisely enough then that variation is captured as well by <span class="math inline">\(\epsilon\)</span>.</li>
</ol>
<p>Now, let’s assume that we to incorporate another variable (say the number of cylinders <span class="math inline">\(c\)</span>), because we think that that variable is very important in explaining miles per gallon. Then we get the following <em>multivariate</em> regression model: <span class="math display">\[
y_i = \alpha + \beta_1 x_i + \beta_2 c_i + \epsilon_i,
\]</span> where we now have two variables on the right hand side (<span class="math inline">\(x_i\)</span> and <span class="math inline">\(c_i\)</span>) and three parameters (<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>). In effect nothing changes with the intuition behind the model. Except for the interpretation of the parameter <span class="math inline">\(\beta_1\)</span> (and thus also <span class="math inline">\(\beta_2\)</span>). Parameter <span class="math inline">\(\beta_1\)</span> now measures the impact of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> <em>holding <span class="math inline">\(c\)</span> constant</em>. So, multivariate regression models is nothing more (and less) than controlling for other factors. And we see later why that is very important.</p>
</div>
<div id="the-least-squares-assumptions" class="section level3">
<h3><span class="header-section-number">4.1.2</span> The least squares assumptions</h3>
<p>We are interested in the effect of the weight of the car on fuel efficiency and,therefore, our <strong>estimate</strong> of <span class="math inline">\(\beta_1\)</span> should be very close to the <strong>true</strong> <span class="math inline">\(\beta_1\)</span>, especially when we have a large number of observations. Regression is great and utterly brilliant in finding this estimate, as long as the following three least squares assumptions hold:</p>
<ol style="list-style-type: decimal">
<li>There are no large outliers</li>
<li>All left hand side (in this case <span class="math inline">\(y\)</span>) and right side variables (in this case <span class="math inline">\(x\)</span> and <span class="math inline">\(c\)</span>) are <em>i.i.d.</em></li>
<li>For the error term the following must hold: <span class="math inline">\(E[\epsilon|X=x] = 0\)</span>.</li>
</ol>
<p>The first one is easy to understand. OLS is very sensitive to large outliers in the dataset. It is therefore always good to look for outliers and think whether they are <em>real</em> observations or perhaps typo’s (in Excel or something). Do not throw outliers immediately away but check whether they are correct.</p>
<p>The second is relatively easy to understand as well (but not that much in practice). <em>i.i.d.</em> in this case stands for independently and identically distributed. This basically means that the observations in your dataset are independent from each other: in other words, the observations should have been correctly <em>sampled</em>.</p>
<p>The third looks the most horrible, and, to be quite honest, is so–both in theory as in reality. This is also the assummption that is least well understood. And especially in assessing whether regression output is correct (is your estimate <em>really</em> close to <span class="math inline">\(\beta_1\)</span>) this assumption is crucial.</p>
</div>
<div id="possible-violations-and-how-to-spot-them" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Possible violations and how to spot them</h3>
<p>A violation of the first assumption is usually easy to spot. There is a very strange outlier. But this also marks the importance of analysing <em>descriptive statistics</em>, including means, maximums and minimums, scatterplots and histograms.</p>
<p>Whether your data is not <em>i.i.d.</em> can come because of a couple of reasons. The most straightforward is getting your data via snowballing (asking your friends and families using facebook to fill in a questionnaire and to ask their friends and families to do so as well). Usually, this means that you have a very specific sample and that the estimate you get is not close to the true value for the whole population. Observations might also be dependent upon each other, because of unobserved factors. In our case, it might be that a type of cars (American) are less fuel efficient than other cars (European).</p>
<p>Another typical violation of the <em>i.i.d.</em> assumption is in time-series, where what happened in the previous period might have a large effect on the current period.</p>
<p>In general, however, violations of the <em>i.i.d.</em> property are not that devestating for your model as long as you are only interested in finding the true <span class="math inline">\(\beta_1\)</span>: namely, it usually only affects the precision of your estimate (the standard error) and not the estimate itself. When this assumption is violated, we therefore say that the estimate is <strong>inefficient</strong>. When you want to predict however, this assumption is crucial, as you would like your estimate to be <em>as precise</em> as possible.</p>
<p>When the third assumption is violated, we say that our estimate is <strong>biased</strong>, in other words <strong>wrong</strong>: our parameter estimate does not come close to the true parameter. And this happens more often than not. So, what does <span class="math inline">\(E[\epsilon|X=x] = 0\)</span> actually mean. Loosely speaking, the parameters on the right hand side (in our case <span class="math inline">\(x\)</span> and <span class="math inline">\(c\)</span>) and the error term (<span class="math inline">\(\epsilon\)</span>) are not allowed to be correlated. There are several ways that this might happen, from which the following are in our case the most relevant:</p>
<ol style="list-style-type: decimal">
<li><p>Reverse causality: <span class="math inline">\(x\)</span> impacts <span class="math inline">\(y\)</span>, but <span class="math inline">\(y\)</span> might impact <span class="math inline">\(x\)</span> as well. This is the classical chicken and egg problem. Do firm perform better because of good management, or do good managers go to the better performing firms?</p></li>
<li><p>Unobserved heterogeneity bias: there are factors that are not in the model but influence both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. For example, if american cars have both an influence on fuel efficiency and weight of the cars then the estimate that we find is not close to the true value of <span class="math inline">\(\beta_1\)</span>.</p></li>
<li><p>Misspecification: we assume that our model is linear, but it actually is not. Then, again, our estimate that we find is not close to the true value of <span class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p>There are other sources of violations, but in this case, these are the most important ones. Reverse causality is usually hard to correct for, but unobserved heterogeneity bias is luckily easier. Namely, we add <em>relevant</em> control variables (as we did with <span class="math inline">\(c\)</span>). In this case we can <em>minimize</em> possible unobserved heterogeity bias. Misspefications are in general as well relative easy to correct for. From our descriptive statistics and scatterplot we usually can infer the relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and control for possible nonlinearities by using (natural) logarithms and squared terms.</p>
<p>As a sidenote, natural logarithms are the ones most used for various reasons not discussed here. If we take the logarithm of both sides then for our univariate regression we get: <span class="math display">\[
\ln(y_i) = \alpha + \beta \ln(x_i) + \epsilon_i,
\]</span> and all the aforementioned rules and assumptions still apply. But there is something peculiar to this regression. Namely, if we are interested in the marginal effect (<span class="math inline">\(\frac{\partial y}{\partial x}\)</span>), we get the following: <span class="math display">\[
\frac{\partial y}{\partial x} = \frac{\partial e^{\ln(y_t)}}{\partial x} = \frac{\partial e^{(\alpha + \beta \ln(x_i) + \epsilon_i)}}{\partial x} = \frac{\beta}{x}e^{(\alpha + \beta \ln(x_i) + \epsilon_i)} = \frac{\beta}{x}y, 
\]</span> In other words: <span class="math display">\[
\beta = \frac{\partial y}{\partial x} \frac{x}{y}
\]</span> which is simply the <strong>elasticity</strong> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. So, if there are logarithms on both the left and right hand side then the parameters (the <span class="math inline">\(\beta\)</span>’s) denotes elasticities.</p>
</div>
<div id="normality-heteroskedasticity-and-multicollinearity" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Normality, heteroskedasticity and multicollinearity</h3>
<p>Until now, we have not discussed the concepts normality, heteroskedasticity and multicollinearity. That is simply because they are not that relevant (as long as we have enough observations, typically above 40). The validity of OLS hinges just upon the three assumptions mentioned above (and they are already difficult enough). In fact, if the three assumptions are satisfied, then, as an <strong>outcome</strong>, the parameters (<span class="math inline">\(\beta\)</span>) are normally distributed. It goes to far to explain why (the theorems required for this are deeply fundamental to statistics), but in any case, normality is <strong>not</strong> a core OLS assumption. It would be nice if both <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are normally distributed because then the standard errors are minimized, but again, whether the estimate of <span class="math inline">\(\beta\)</span> you find is correct or not (biased or unbiased) does not depend on normality <strong>assumptions</strong>.</p>
<p>Heteroskedasticy (in other words your standard errors are not constant) as well leads to quite some confusion. In general heteroskedasticity only leads to inefficient estimates (so only affects the standard errors). Nothing more, nothing less. And there are corrections for that (robust standard errors in <code>Stata</code> and similar procedures in <code>R</code>), so that nobody needs to care anymore about heteroskedasticity.</p>
<p>Finally, there is multicollinearity. And this come in two flavours: perfect and imperfect multicollinearity. Perfect multicollinearity occurs, e.g., when your model contains two identical variables. Then, OLS can not decide which one to use and usually one of the variables is dropped, or your computer program gives an error (<em>computer says no</em>).</p>
<p>Imperfect multicollinearity occurs when two variables are highly (but not perfectly) correlated. This occurs less often than one may think. Variables that are highly correlated (say <span class="math inline">\(age\)</span> and <span class="math inline">\(age^2\)</span>) can be perfectly incorporated in a model. Only when the correlation becomes very high (say above 95% or even higher) then something strange happens: the standard error get very large. Why? That is because of the definition mentioned above. Parameter <span class="math inline">\(\beta_1\)</span> measures the impact of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> <em>holding <span class="math inline">\(c\)</span> constant</em>. But if <span class="math inline">\(x\)</span> and <span class="math inline">\(c\)</span> are very correlated and you control for <span class="math inline">\(c\)</span>, then not much variation is left over for <span class="math inline">\(x\)</span>. So, <span class="math inline">\(c\)</span> actually removes the variation of <span class="math inline">\(x\)</span>. This always happens, and there is trade-off between adding more variables and leaving enough variation (note that there is always correlation between variables), but usually it all goes fine. Good judgement and sound thinking typically helps more than strange statistics (VIF?).</p>
</div>
</div>
<div id="sec:applications" class="section level2">
<h2><span class="header-section-number">4.2</span> Applications of linear regression</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="using-r-in-social-science-research.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="network-analysis-with-r.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
